{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SaDYMWrsvPi",
        "outputId": "2115a742-b10b-4ecb-bae6-b1fc111e5d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 46 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 44.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845513 sha256=3269bcb38bbc3e3db914f4509373c37a592920fda83552b88feae75ca90ae3c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##What is UDF?\n",
        "\n",
        "UDF’s a.k.a User Defined Functions, If you are coming from SQL background, UDF’s are nothing new to you as most of the traditional RDBMS databases support User Defined Functions, these functions need to register in the database library and use them on SQL as regular functions."
      ],
      "metadata": {
        "id": "i32DZaQss93R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark UDF’s are similar to UDF on traditional databases. In PySpark, you create a function in a Python syntax and wrap it with PySpark SQL udf() or register it as udf and use it on DataFrame and SQL respectively."
      ],
      "metadata": {
        "id": "OWrmaLt7tAd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Why do we need a UDF?\n",
        "\n",
        "UDF’s are used to extend the functions of the framework and re-use these functions on multiple DataFrame’s. \n",
        "\n",
        "For example, you wanted to convert every first letter of a word in a name string to a capital case; PySpark build-in features don’t have this function hence you can create it a UDF and reuse this as needed on many Data Frames. \n",
        "\n",
        "UDF’s are once created they can be re-used on several DataFrame’s and SQL expressions."
      ],
      "metadata": {
        "id": "xx9Gd92ytDwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: When you creating UDF’s you need to design them very carefully otherwise you will come across optimization & performance issues."
      ],
      "metadata": {
        "id": "Kakv5hcBtTsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "M7A-hnpZuRXt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('SparkUDF').getOrCreate()\n",
        "\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\"),\n",
        "    (\"4\", \"Madhav Madhav\"),\n",
        "    (\"5\", \"Yash Yash\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMlLIXKgtaaN",
        "outputId": "8bae5d09-3f4e-45c7-eccd-68bb18d2b36e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|Seqno|Name         |\n",
            "+-----+-------------+\n",
            "|1    |john jones   |\n",
            "|2    |tracey smith |\n",
            "|3    |amy sanders  |\n",
            "|4    |Madhav Madhav|\n",
            "|5    |Yash Yash    |\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step in creating a UDF is creating a Python function. \n",
        "\n",
        "Below snippet creates a function convertCase() which takes a string parameter and converts the first letter of every word to capital letter. \n",
        "\n",
        "UDF’s take parameters of your choice and returns a value."
      ],
      "metadata": {
        "id": "Wth-uPmKvKTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convertCase(str):\n",
        "    resStr=\"\"\n",
        "    arr = str.split(\" \")\n",
        "    for x in arr:\n",
        "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
        "    return resStr "
      ],
      "metadata": {
        "id": "3zn9lkOItaWy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert a Python function to PySpark UDF"
      ],
      "metadata": {
        "id": "uo3oR3_NvkeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Converting function to UDF \"\"\"\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StructType, IntegerType, StringType\n",
        "\n",
        "convertUDF = udf(lambda z: convertCase(z),StringType())"
      ],
      "metadata": {
        "id": "lZxTOCEstaTr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Converting function to UDF \n",
        "StringType() is by default hence not required \"\"\"\n",
        "\n",
        "convertUDF = udf(lambda z: convertCase(z)) \n"
      ],
      "metadata": {
        "id": "pvTTL9DgtaQW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using UDF with DataFrame\n",
        "\n",
        "Using UDF with PySpark DataFrame select()"
      ],
      "metadata": {
        "id": "94zHCDvPwZmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df.select(col(\"Seqno\"), \\\n",
        "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
        "   .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLDWQhZntaKf",
        "outputId": "fddefe02-7a3e-4a6f-d50f-8eaad7361ce1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------+\n",
            "|Seqno|Name          |\n",
            "+-----+--------------+\n",
            "|1    |John Jones    |\n",
            "|2    |Tracey Smith  |\n",
            "|3    |Amy Sanders   |\n",
            "|4    |Madhav Madhav |\n",
            "|5    |Yash Yash     |\n",
            "+-----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using UDF with PySpark DataFrame withColumn()"
      ],
      "metadata": {
        "id": "qTf9UFAjw_HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upperCase(str):\n",
        "    return str.upper()"
      ],
      "metadata": {
        "id": "TcVMssPwtZ_u"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s convert upperCase() python function to UDF and then use it with DataFrame withColumn(). \n",
        "\n",
        "Below example converts the values of “Name” column to upper case and creates a new column “Curated Name”"
      ],
      "metadata": {
        "id": "ycork8DYxHHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upperCaseUDF = udf(lambda z:upperCase(z),StringType())   \n",
        "\n",
        "\n",
        "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROemAMaAs4SR",
        "outputId": "cbe13cae-2d28-4bbc-dffe-a75493c75e25"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+-------------+\n",
            "|Seqno|Name         |Cureated Name|\n",
            "+-----+-------------+-------------+\n",
            "|1    |john jones   |JOHN JONES   |\n",
            "|2    |tracey smith |TRACEY SMITH |\n",
            "|3    |amy sanders  |AMY SANDERS  |\n",
            "|4    |Madhav Madhav|MADHAV MADHAV|\n",
            "|5    |Yash Yash    |YASH YASH    |\n",
            "+-----+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Using UDF on SQL \"\"\"\n",
        "\n",
        "spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
        "\n",
        "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
        "\n",
        "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\").show(truncate=False)     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAn42AwJzbzw",
        "outputId": "c7d08842-eb13-4144-8357-c0cd47a7d5b3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------+\n",
            "|Seqno|Name          |\n",
            "+-----+--------------+\n",
            "|1    |John Jones    |\n",
            "|2    |Tracey Smith  |\n",
            "|3    |Amy Sanders   |\n",
            "|4    |Madhav Madhav |\n",
            "|5    |Yash Yash     |\n",
            "+-----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE \" + \\\n",
        "          \"where Name is not null and convertUDF(Name) like '%John%'\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7ROeEm69UmX",
        "outputId": "c5a42202-1191-45a6-eb10-8a116d4e42bb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+\n",
            "|Seqno|Name       |\n",
            "+-----+-----------+\n",
            "|1    |John Jones |\n",
            "+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" null check \"\"\"\n",
        "\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\"),\n",
        "    (\"6\",None)]"
      ],
      "metadata": {
        "id": "NJw_j0ob9j4k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.createDataFrame(data=data,schema=columns)\n",
        "df2.show(truncate=False)\n",
        "df2.createOrReplaceTempView(\"NAME_TABLE2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMrfd8xc94k_",
        "outputId": "a51c6955-ae39-410d-d242-f788331065d5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|Seqno|Name        |\n",
            "+-----+------------+\n",
            "|1    |john jones  |\n",
            "|2    |tracey smith|\n",
            "|3    |amy sanders |\n",
            "|6    |null        |\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , StringType())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHFD9Cj799uR",
        "outputId": "a5cb6871-2c12-4c39-aa6f-f022615f6bd4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.<lambda>(str)>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GodfKIR-yCQ",
        "outputId": "6147c17c-cdc4-41ea-f17b-65810d3fbac5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|_nullsafeUDF(Name)|\n",
            "+------------------+\n",
            "|John Jones        |\n",
            "|Tracey Smith      |\n",
            "|Amy Sanders       |\n",
            "|                  |\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
        "          \" where Name is not null and _nullsafeUDF(Name) like '%Tra%'\").show(truncate=False) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AQlXTsO-1z3",
        "outputId": "2462307c-90b5-4ba5-fd6a-fc9ffa40d75f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|Seqno|Name         |\n",
            "+-----+-------------+\n",
            "|2    |Tracey Smith |\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AktXGPG-7M-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}